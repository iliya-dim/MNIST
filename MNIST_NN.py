# -*- coding: utf-8 -*-
"""MNIST_Data_Augmentation_by_Affine_MonteCarlo_NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WW2zN3jGQDVhbPejlfC7H1RZ_xys3yNx
"""

from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', version=1)

train_border = int(6e4)
X_train, X_test = mnist["data"][:train_border], mnist["data"][train_border:]
y_train, y_test = mnist["target"][:train_border], mnist["target"][train_border:]

print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt

import numpy as np
np.random.seed(42)

import time
t_start = time.time()

#Create additional dataset by using KNN classifier to denoise the original dataset
from sklearn.neighbors import KNeighborsClassifier

denoised_border = int(5e3) #<=5e4 - use train set only

def knn_denoised_augmentation(data=X_train[:denoised_border], labels=y_train[:denoised_border],
                                    noise_level=100, plot_sample=False):

  noise = np.random.randint(0, noise_level, size=(data.shape))
  data_noised = data + noise

  knn = KNeighborsClassifier()
  knn.fit(data_noised, data)
  data_transformed = knn.predict(data_noised)

  if plot_sample==True:
    plt.figure(figsize=(12, 3))

    plt.subplot(131)
    plt.title("Original: {}".format(y_train[0]), fontsize=12)
    plt.imshow(data[0].reshape(28,28), cmap='Greys')

    plt.subplot(132)
    plt.title("Noisy: {}".format(y_train[0]), fontsize=12)
    plt.imshow(data_noised[0].reshape(28, 28), cmap='Greys')

    plt.subplot(133)
    plt.title("KNN denoised: {}".format(y_train[0]), fontsize=12)
    plt.imshow(data_transformed[0].reshape(28, 28), cmap='Greys')

  return data_transformed, labels

X_denoised, y_denoised = knn_denoised_augmentation(plot_sample=True)
print(X_denoised.shape, y_denoised.shape)

#Generate arrays with parameters for affine transformation for each image in the dataset
def generate_affine_transform_parameters(sample_size=40, dropout_rate=0.2,
                                   shear_boundary=0.1, scale_boundary=0.1,
                                   shift_boundary=5, rotation_boundary=15,
                                   print_mask=False):
  
  #Generate a mask array to randomly drop transformations with probability p
  mask = np.random.choice([0, 1], size=(sample_size, 5),
                          p=[dropout_rate, 1-dropout_rate])
  
  if print_mask==True: print(mask)

  #Seek rows where all mask values==0; no transformation will be applied to sample --> duplicating samples
  rows = (mask.sum(1) == 0).astype(np.uint8)

  #Create array of zero-row positions
  row_idx = np.arange(1, len(rows)+1)*rows #if np.arange(0, ...) empty 0's row is missed

  #Turn positions into index array
  row_idx = row_idx[row_idx>0] - 1

  #Choose a random column index in the zero rows --> corresponds to affine transformation
  col_idx = np.random.randint(5, size=len(row_idx))

  #Switch one entry in zero rows to one
  mask[row_idx, col_idx] = 1

  if print_mask==True:
    print("Row is all 0's:\n", rows) #binary array
    print("Row index where all entries are 0:\n", row_idx)
    print("Random column index in zero rows:\n", col_idx)
    print("New mask:\n", mask)
    print("Sum of zero rows in new mask: ", (mask.sum(1)==0).sum())

  shears = np.random.uniform(-shear_boundary, shear_boundary, size=sample_size) #Generate transformation range
  shears = shears*mask[:, 0] #Mask the transformation range

  scales = np.random.uniform(1-scale_boundary, 1+scale_boundary, size=sample_size)
  scales = scales*mask[:, 1]
  scales = np.where(scales, scales, 1) #No scaling is multiplication by 1, not 0

  rotations = np.random.randint(-rotation_boundary, rotation_boundary, size=sample_size)
  rotations = rotations*mask[:, 2]

  shifts = np.random.randint(-shift_boundary, shift_boundary, size=(sample_size, 2))
  shifts = shifts*mask[:, 3:]

  return {'shears': shears, 'scales': scales,
          'rotations': rotations, 'shifts': shifts}

#Create affine transformations based on previously generated arrays
#If apply_to_all=False all (different) transformations are applied to the same sample
from skimage.transform import warp, AffineTransform

def apply_affine_transform(shears, scales, shifts, rotations, data=X_train, labels=y_train,
                            data_special=0, sample_size=40, apply_to_all=True):
  
  data = data.reshape(-1, 28, 28)
  data_transformed, label_transformed = [], []

  for i in range(sample_size):
    tform = AffineTransform (shear=shears[i], scale=(scales[i], scales[i]),
                            translation=shifts[i], rotation=rotations[i]*(np.pi/180))
    
    if apply_to_all == True:
      data_transformed.append(warp(data[i], tform, output_shape=(28, 28)))
    else:
      data_transformed.append(warp(data[data_special], tform, output_shape=(28, 28)))
      
    label_transformed.append("Original: {}\nShear: {:.2f}\nScale: {:.2f}\nTransl: {}\nRotation: {}".
                             format(labels[i], shears[i], scales[i], shifts[i], rotations[i]))

  return np.array(data_transformed).reshape(-1, 784), label_transformed

boundaries = {'shear_boundary': 0.1, 'scale_boundary': 0.1,
              'shift_boundary': 3, 'rotation_boundary': 5}

transform_params = generate_affine_transform_parameters(**boundaries, sample_size=denoised_border)

# Create augmented dataset and labels
X_augmented, y_augmented = apply_affine_transform(**transform_params, sample_size=denoised_border)

#Normalize to 1
X_train /= 255
X_denoised /= 255
X_augmented /= 255

#Create datasets with combined data
X_original_denoised = np.concatenate((X_train[:denoised_border], X_denoised))
y_original_denoised = np.concatenate((y_train[:denoised_border], y_denoised))
print("Orginal + Denoised shapes: \t", X_original_denoised.shape, y_original_denoised.shape)

X_original_augmented = np.concatenate((X_train[:denoised_border], X_augmented))
y_original_augmented = np.concatenate((y_train[:denoised_border], y_train[:denoised_border]))
print("Orginal + Affine shapes: \t", X_original_augmented.shape, y_original_augmented.shape)

X_original_denoised_augmented = np.concatenate((X_train[:denoised_border], X_denoised, X_augmented))
y_original_denoised_augmented = np.concatenate((y_train[:denoised_border], y_denoised, y_train[:denoised_border]))
print("Orginal + Denoised + Augmented shapes: \t", X_original_augmented.shape, y_original_augmented.shape)

data_list = [X_train[:denoised_border], X_denoised, X_augmented, X_original_denoised, X_original_augmented, X_original_denoised_augmented]
labels_list = [y_train[:denoised_border], y_denoised, y_train[:denoised_border], y_original_denoised, y_original_augmented, y_original_denoised_augmented]
dataset_names = ["Original MNIST", "Denoised", "Affine Augmented", "Original+Denoised", "Original+Augmented", "Original+Denoised+Augmented"]

import tensorflow as tf
from tensorflow import keras
from keras.metrics import Precision, Recall

#Convert training labels to One-Hot Encoded label (else use loss='sparse_categorical_crossentropy' in model.compile())
test_labels = keras.utils.to_categorical(y_test)
train_labels = keras.utils.to_categorical(y_train)
train_labels_list = [keras.utils.to_categorical(label) for label in labels_list]

#Build a simple model with one hidden layer with 512 neurons - use to get initial estimates, precision and recall
def get_nn():
    model = keras.models.Sequential(
        [keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal', input_shape=(784,)),
         keras.layers.Dense(10, activation='softmax')]
         )
    
    model.compile(
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy', Precision(), Recall()])
    
    return model

#Build a model with variable number of hidden layers and neurons to use for architecture optimization
def build_model(n_hidden=2, n_neurons=500, act_f='relu', init_w='he_normal', lr=1e-3,
                l2_const=1e-2, add_bn=False, add_dropout=False, dr_rate=0.2, add_conv=False, n_conv=2):
  
      model = keras.models.Sequential()
      
      if add_conv:
          model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
          for conv_layer in range(n_conv):
              model.add(keras.layers.MaxPool2D((2, 2)))
              model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))

      model.add(keras.layers.Flatten())

      for layer in range(n_hidden):
          model.add(keras.layers.Dense(n_neurons, activation=act_f, kernel_initializer=init_w,
                                       kernel_regularizer = keras.regularizers.l2(l2_const)))
          if add_bn:
              model.add(keras.layers.BatchNormalization())
          if add_dropout:
              if act_f=='selu':
                  model.add(keras.layers.AlphaDropout(rate=dr_rate)) #preserves mean and variance of its inputs to ensure self-normalization
              else:
                  model.add(keras.layers.Dropout(rate=dr_rate))
    
      model.add(keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform'))

      my_optimizer = keras.optimizers.RMSprop(learning_rate=lr)
      model.compile(
          optimizer=my_optimizer,
          loss='categorical_crossentropy',
          metrics=['accuracy'])
      
      return model

#Fit and evaluate on different datasets
histories_partial, results_partial = [], []

for data, labels in zip(data_list, train_labels_list):
    keras.backend.clear_session()
    network = get_nn()

    histories_partial.append(
        network.fit(data, labels, epochs=30, batch_size=32, validation_split=0.1))

    results_partial.append(
        network.evaluate(X_test, test_labels))


results_partial = np.array(results_partial)

#Plot accuracy and loss with epoch for each dataset
def plot_results(histories=histories_partial, rows=2, cols=3, v_line=False, x_vertical=10):
  fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 10))

  for i in range(rows):
    for j in range(cols):
      axes[i, j].plot(histories[i+2*j].epoch, histories[i+2*j].history['accuracy'], 'g-', label='accuracy')
      axes[i, j].plot(histories[i+2*j].epoch, histories[i+2*j].history['val_accuracy'], 'g--', label='val_accuracy')
      axes[i, j].plot(histories[i+2*j].epoch, histories[i+2*j].history['loss'], 'r-', label='loss')
      axes[i, j].plot(histories[i+2*j].epoch, histories[i+2*j].history['val_loss'], 'r--', label='val_loss')

      if v_line: axes[i, j].plot([x_vertical, x_vertical], [0, 1], 'b--', label='lr_reduction')
      
      axes[i, j].set_xlabel('epochs')
      axes[i, j].set_ylim(0, 1.1)
      axes[i, j].set_title(dataset_names[i+2*j])
      axes[i, j].grid()
      axes[i, j].legend()

plot_results()

#Print evaluation on test dataset with scheduled learning rate
print("{:30} {:8}   {:8}   {:8}".format("Data", "Accuracy", "Precision", "Recall"))
for name, (acc, pre, rec) in zip(dataset_names, results_partial[:, 1:]):
  print("{:<30} {:.6f}   {:.6f}   {:.6f}".format(name, acc, pre, rec))

keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

#Define a scheduling function to reduce the learning rate with epochs
def scheduler(epoch, lr):
  if epoch<5:
    return lr
  else:
    return lr*tf.math.exp(-0.3)

callback_lr = keras.callbacks.LearningRateScheduler(scheduler)

#Fit and evaluate with scheduled learning rate
histories_partial_lr, results_partial_lr = [], []

for data, labels in zip(data_list, train_labels_list):
    keras.backend.clear_session()
    network = get_nn()

    histories_partial_lr.append(
        network.fit(data, labels, epochs=30, batch_size=32, validation_split=0.1,
                    callbacks=[callback_lr]))  
    
    results_partial_lr.append(
        network.evaluate(X_test, test_labels))


results_partial_lr = np.array(results_partial_lr)

#Plot the results of the simple model using learning rate scheduler after 5th epoch
plot_results(histories_partial_lr, v_line=True, x_vertical=5)

#Print evaluation on test dataset with scheduled learning rate
print("{:30} {:8}   {:8}   {:8}".format("Data", "Accuracy", "Precision", "Recall"))
for name, (acc, pre, rec) in zip(dataset_names, results_partial_lr[:, 1:]):
  print("{:<30} {:.6f}   {:.6f}   {:.6f}".format(name, acc, pre, rec))

#Use a model with n_hidden=2, n_neurons=500
histories_partial_bm, results_partial_bm = [], []

for data, labels in zip(data_list, train_labels_list):
    keras.backend.clear_session()
    network = build_model()

    histories_partial_bm.append(
        network.fit(data, labels, epochs=30, batch_size=32, validation_split=0.1,
                    callbacks=[callback_lr, keras.callbacks.EarlyStopping(patience=10)]))  
    
    results_partial_bm.append(
        network.evaluate(X_test, test_labels))


results_partial_bm = np.array(results_partial_bm)

#Plot the results of the simple model using learning rate scheduler after 5th epoch
plot_results(histories_partial_bm, v_line=True, x_vertical=5)

#Print evaluation on test dataset with scheduled learning rate
print("{:30} {:8}".format("Data", "Accuracy"))
for name, acc in zip(dataset_names, results_partial_bm[:, 1]):
  print("{:<30} {:.4f}".format(name, acc))

#Test with scaled eponential linear unit activation
histories_partial_selu, results_partial_selu = [], []

for data, labels in zip(data_list, train_labels_list):
    keras.backend.clear_session()
    network = build_model(act_f='selu', init_w='lecun_normal', l2_const=0) #l2_const=0 to avoid l2 normalization
    
    histories_partial_selu.append(
        network.fit(data, labels, epochs=30, batch_size=32, validation_split=0.1,
                    callbacks=[callback_lr, keras.callbacks.EarlyStopping(patience=10)])
        )  
    
    results_partial_selu.append(
        network.evaluate(X_test, test_labels)
        )

results_partial_selu = np.array(results_partial_selu)

plot_results(histories_partial_selu, v_line=True, x_vertical=5)

#Use a model with n_hidden=2, n_neurons=500
histories_partial_conv, results_partial_conv = [], []

for data, labels in zip(data_list, train_labels_list):
    keras.backend.clear_session()
    network = build_model(add_conv=True, l2_const=0)

    histories_partial_conv.append(
        network.fit(data.reshape(-1, 28, 28, 1), labels, epochs=30, batch_size=32, validation_split=0.1,
                    callbacks=[callback_lr, keras.callbacks.EarlyStopping(patience=10)])
        ) #reshape() to match conv2D input
    
    results_partial_conv.append(
        network.evaluate(X_test.reshape(-1, 28, 28, 1), test_labels)
        )

results_partial_conv = np.array(results_partial_conv)

plot_results(histories_partial_conv, v_line=True, x_vertical=5)

# Import a model saved on MyDrive
from google.colab import drive
drive.mount("content")

grid_model = keras.models.load_model("content/My Drive/Colab Notebooks/Trial_Models/MNIST_GridSearch_best.h5")
grid_model.summary()

print([layer.trainable for layer in grid_model.layers])

#Test with scaled eponential linear unit activation
histories_partial_clone, results_partial_clone = [], []

for data, labels in zip(data_list, train_labels_list):
    keras.backend.clear_session()

    grid_model_clone = keras.models.clone_model(grid_model)
    grid_model_clone.set_weights(grid_model.get_weights())
    
    grid_model_clone.compile(
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    histories_partial_clone.append(
        grid_model_clone.fit(data, labels, epochs=30, batch_size=32, validation_split=0.1,
                             callbacks=[callback_lr, keras.callbacks.EarlyStopping(patience=10)]))      
    
    results_partial_clone.append(grid_model_clone.evaluate(X_test, test_labels))

results_partial_clone = np.array(results_partial_clone)

plot_results(histories_partial_clone, v_line=True, x_vertical=5)

#Test with scaled eponential linear unit activation
histories_partial_clone, results_partial_clone = [], []

for data, labels in zip(data_list, train_labels_list):
    keras.backend.clear_session()

    grid_model_clone = keras.models.clone_model(grid_model)
    grid_model_clone.set_weights(grid_model.get_weights())

    # Freeze first layer
    grid_model_clone.layers[0].trainable=False
    
    grid_model_clone.compile(
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    histories_partial_clone.append(
        grid_model_clone.fit(data, labels, epochs=30, batch_size=32, validation_split=0.1,
                             callbacks=[callback_lr, keras.callbacks.EarlyStopping(patience=10)]))      
    
    results_partial_clone.append(grid_model_clone.evaluate(X_test, test_labels))

results_partial_clone = np.array(results_partial_clone)

plot_results(histories_partial_clone, v_line=True, x_vertical=5)