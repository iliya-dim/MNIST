# -*- coding: utf-8 -*-
"""MNIST_Data_Augmentation_by_Affine_MonteCarlo_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1crVATwFT3G_zH_sjKEMgvknAx0hy8BvK
"""

from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', version=1)

train_border = int(6e4)
X_train, X_test = mnist["data"][:train_border], mnist["data"][train_border:]
y_train, y_test = mnist["target"][:train_border], mnist["target"][train_border:]

print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt

import numpy as np
np.random.seed(42)

import time

#Show the first three images
plt.figure(figsize=(12, 3))

plt.subplot(131)
plt.title(y_train[0], fontsize=12)
plt.imshow(X_train[0].reshape(28, 28), cmap='Greys')

plt.subplot(132)
plt.title(y_train[1], fontsize=12)
plt.imshow(X_train[1].reshape(28, 28), cmap='Greys')

plt.subplot(133)
plt.title(y_train[2], fontsize=12)
plt.imshow(X_train[2].reshape(28, 28), cmap='Greys')

#Plot a slice of the image cut along pixel values cut_x and cut_y
def profile_image(image = X_train[0], cut_x = 13, cut_y=13, min_pixel_value=0,
                 plot_profile=True, print_profile=True):
  
  image = image.reshape(28, 28)
  x_profile = image[cut_y]
  y_profile = image[:, cut_x]

  #Show which pixel positions are occupied with values > min_pixel_value
  x_occupied = np.where(x_profile > min_pixel_value, 1, 0)
  y_occupied = np.where(y_profile > min_pixel_value, 1, 0)

  #Sum all pixels and divide by non-zero values
  mean_x = x_profile.sum() / x_occupied.sum()
  mean_y = y_profile.sum() / y_occupied.sum()

  #Find boundary pixels
  x_boundaries = [(i, i+1) for i in range(len(x_profile)-1) if x_occupied[i+1]-x_occupied[i]]
  y_boundaries = [(i, i+1) for i in range(len(y_profile)-1) if y_occupied[i+1]-y_occupied[i]]

  #Calculate the difference between pixels at the boundary locations
  x_steps = [np.abs(x_profile[boundary[1]] - x_profile[boundary[0]]) for boundary in x_boundaries]
  y_steps = [np.abs(y_profile[boundary[1]] - y_profile[boundary[0]]) for boundary in y_boundaries]

  #Calculate mean step values
  x_steps_mean = int(np.array(x_steps).mean())
  y_steps_mean = int(np.array(y_steps).mean())
  xy_steps_mean = int((x_steps_mean + y_steps_mean)/2)

  if plot_profile == True:
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4.5), tight_layout=True)
    #fig.suptitle("Image {}", fontsize=16) #Could be used if label argument is supplied

    ax1.plot(list(range(image.shape[1])), x_profile, drawstyle='steps-post', label='X')
    ax1.set_title('X-profile at y={}'.format(cut_y))
    ax1.set_xlabel('X pixel')
    ax1.set_ylabel('Profile height')

    ax2.plot(list(range(image.shape[0])), y_profile, drawstyle='steps-post', label='Y')
    ax2.set_title('Y-profile at x={}'.format(cut_x))
    ax2.set_xlabel('Y pixel')
    ax2.set_ylabel('Profile height')

    plt.show()

  if print_profile == True:
    print("X-profile at y={}:\n".format(cut_y), x_profile)
    print()
    print("Y-profile at x={}:\n".format(cut_x), y_profile)
    print()

    print("X-profile mean: \t", mean_x)
    print("X-profile boundaries: \t", x_boundaries)
    print("X-profile steps: \t", x_steps)
    print("X-profile steps mean: \t", x_steps_mean)
    print()

    print("Y-profile mean: \t", mean_y)
    print("Y-profile boundaries: \t", y_boundaries)
    print("Y-profile steps: \t", y_steps)
    print("Y-profile steps mean: \t", y_steps_mean)
    print()

    print("XY steps mean: \t", xy_steps_mean)

  return {'xy profiles': (x_profile, y_profile),
            'xy occupied pixels': (x_occupied, y_occupied),
            'xy pixel boundaries': (x_boundaries, y_boundaries),
            'xy boundary steps': (x_steps, y_steps),
            'xy boundary steps_means': (x_steps_mean, y_steps_mean, xy_steps_mean)}

d = profile_image()
print(d.keys())

_ = profile_image(image=X_train[1], cut_x=7)

#Create additional dataset by using KNN classifier to denoise the original dataset
from sklearn.neighbors import KNeighborsClassifier

denoised_border = int(5e3) #<=5e4 - use train set only

def knn_denoised_augmentation(data=X_train[:denoised_border], labels=y_train[:denoised_border],
                                    noise_level=100, plot_sample=False):

  noise = np.random.randint(0, noise_level, size=(data.shape))
  data_noised = data + noise

  knn = KNeighborsClassifier()
  knn.fit(data_noised, data)
  data_transformed = knn.predict(data_noised)

  if plot_sample==True:
    plt.figure(figsize=(12, 3))

    plt.subplot(131)
    plt.title("Original: {}".format(y_train[0]), fontsize=12)
    plt.imshow(data[0].reshape(28,28), cmap='Greys')

    plt.subplot(132)
    plt.title("Noisy: {}".format(y_train[0]), fontsize=12)
    plt.imshow(data_noised[0].reshape(28, 28), cmap='Greys')

    plt.subplot(133)
    plt.title("KNN denoised: {}".format(y_train[0]), fontsize=12)
    plt.imshow(data_transformed[0].reshape(28, 28), cmap='Greys')

  return data_transformed, labels

X_denoised, y_denoised = knn_denoised_augmentation(plot_sample=True)
print(X_denoised.shape, y_denoised.shape)

#Return pixel-by-pixel difference between image and transformation
def calculate_difference (X_initial, X_transformed, sample_size=40,
                          apply_to_all=True, sample_to_apply=0):

  if apply_to_all == True: #Set False only if X_new was generated with apply_to_all=False
    return X_transformed[:sample_size] - X_initial[:sample_size]
  else:
    return X_transformed[:sample_size] - X_initial[sample_to_apply]


X_diff = calculate_difference(X_train, X_denoised)
plt.title("Pixel difference between\noriginal {} and denoised".format(y_train[0]), fontsize=12)
plt.imshow(X_diff[0].reshape(28, 28))
plt.colorbar()

#Return number of different pixels in the image
def calc_num_diff_pixels(image_difference = X_diff):
  return np.where(image_difference, 1, 0).sum(1)


num_diff_pix_per_image = calc_num_diff_pixels(X_diff)
num_diff_frac = np.round(num_diff_pix_per_image*100/784, decimals=1) #as % of all

print('Number of different pixels per image: \n', num_diff_pix_per_image[0])
print("Fraction (%) of different pixels in augmented image:\n", num_diff_frac[0])

# Set pixels differing by more than a certain amount to 1; sum over each image
def calc_num_very_diff_pixels(image_difference=X_diff, pixel_difference=10):
  return np.where(np.abs(image_difference) > pixel_difference, 1, 0).sum(1)


X_very_diff = calc_num_very_diff_pixels(pixel_difference=128)
X_very_diff_frac = np.round(X_very_diff*100/784, decimals=1) # in %

print("Number of very different pixels in augmented image:\n", X_very_diff[0])
print("Fraction (%) of very different pixels in augmented image:\n", X_very_diff_frac[0])

#Plot the affine transformations (or other arrays)
def plot_data (data, labels, n_rows=1, n_columns=8, col_map=False):
  
  data = data[:100]
  data = data.reshape(-1, 28, 28)
  fig, axes = plt.subplots(nrows=n_rows, ncols=n_columns, figsize=(n_columns*2, n_rows*3), sharex=True, sharey=True)

  for i in range(n_rows):
    for j in range(n_columns):
      current = i*n_columns + j + 1
      axes[i, j].set_title(labels[current-1])
      if col_map == True:
        axes[i, j].imshow(data[current-1])
      else:
        axes[i, j].imshow(data[current-1], interpolation="nearest", cmap="Greys")


plot_data(X_denoised, labels=y_denoised, n_rows=5, n_columns=8)

plot_data(X_diff, y_denoised, n_rows=5, col_map=True)

#Transform multiple image arrays into a single array; creates single colormap for all digits; from book
def plot_as_one(instances, title, images_per_row=8, figsize=(16, 8), **options):
    size = 28
    images_per_row = min(len(instances), images_per_row)
    n_rows = (len(instances) - 1) // images_per_row + 1
    n_empty = n_rows * images_per_row - len(instances)

    images = [instance.reshape(size,size) for instance in instances]
    row_images = []

    images.append(np.zeros((size, size * n_empty)))
    for row in range(n_rows):
        rimages = images [row*images_per_row : (row + 1)*images_per_row]
        row_images.append(np.concatenate(rimages, axis=1))
    image = np.concatenate(row_images, axis=0)

    fig = plt.figure(figsize=figsize)
    plt.title(title)
    plt.imshow(image)
    plt.axis("off")
    cbar = plt.colorbar()
    cbar.set_label("Pixel intensity (0-256)")

plot_as_one(X_diff, title="Difference Denoised - Original", images_per_row=8)

#Generate arrays with parameters for affine transformation for each image in the dataset
def generate_affine_transform_parameters(sample_size=40, dropout_rate=0.2,
                                   shear_boundary=0.1, scale_boundary=0.1,
                                   shift_boundary=5, rotation_boundary=15,
                                   print_mask=False):
  
  #Generate a mask array to randomly drop transformations with probability p
  mask = np.random.choice([0, 1], size=(sample_size, 5),
                          p=[dropout_rate, 1-dropout_rate])
  
  if print_mask==True: print(mask)

  #Seek rows where all mask values==0; no transformation will be applied to sample --> duplicating samples
  rows = (mask.sum(1) == 0).astype(np.uint8)

  #Create array of zero-row positions
  row_idx = np.arange(1, len(rows)+1)*rows #if np.arange(0, ...) empty 0's row is missed

  #Turn positions into index array
  row_idx = row_idx[row_idx>0] - 1

  #Choose a random column index in the zero rows --> corresponds to affine transformation
  col_idx = np.random.randint(5, size=len(row_idx))

  #Switch one entry in zero rows to one
  mask[row_idx, col_idx] = 1

  if print_mask==True:
    print("Row is all 0's:\n", rows) #binary array
    print("Row index where all entries are 0:\n", row_idx)
    print("Random column index in zero rows:\n", col_idx)
    print("New mask:\n", mask)
    print("Sum of zero rows in new mask: ", (mask.sum(1)==0).sum())

  shears = np.random.uniform(-shear_boundary, shear_boundary, size=sample_size) #Generate transformation range
  shears = shears*mask[:, 0] #Mask the transformation range

  scales = np.random.uniform(1-scale_boundary, 1+scale_boundary, size=sample_size)
  scales = scales*mask[:, 1]
  scales = np.where(scales, scales, 1) #No scaling is multiplication by 1, not 0

  rotations = np.random.randint(-rotation_boundary, rotation_boundary, size=sample_size)
  rotations = rotations*mask[:, 2]

  shifts = np.random.randint(-shift_boundary, shift_boundary, size=(sample_size, 2))
  shifts = shifts*mask[:, 3:]

  return {'shears': shears, 'scales': scales,
          'rotations': rotations, 'shifts': shifts}

boundaries = {'shear_boundary': 0.1, 'scale_boundary': 0.1,
              'shift_boundary': 3, 'rotation_boundary': 5}

transform_params_demonstr = generate_affine_transform_parameters(dropout_rate=0.8, print_mask=True)
transform_params_real = generate_affine_transform_parameters(**boundaries, sample_size=denoised_border)

#Create affine transformations based on previously generated arrays
#If apply_to_all=False all (different) transformations are applied to the same sample
from skimage.transform import warp, AffineTransform

def apply_affine_transform(shears, scales, shifts, rotations, data=X_train, labels=y_train,
                            data_special=0, sample_size=40, apply_to_all=True):
  
  data = data.reshape(-1, 28, 28)
  data_transformed, label_transformed = [], []

  for i in range(sample_size):
    tform = AffineTransform (shear=shears[i], scale=(scales[i], scales[i]),
                            translation=shifts[i], rotation=rotations[i]*(np.pi/180))
    
    if apply_to_all == True:
      data_transformed.append(warp(data[i], tform, output_shape=(28, 28)))
    else:
      data_transformed.append(warp(data[data_special], tform, output_shape=(28, 28)))
      
    label_transformed.append("Original: {}\nShear: {:.2f}\nScale: {:.2f}\nTransl: {}\nRotation: {}".
                             format(labels[i], shears[i], scales[i], shifts[i], rotations[i]))

  return np.array(data_transformed).reshape(-1, 784), label_transformed

X_augmented, y_augmented = apply_affine_transform(**transform_params_real, sample_size=denoised_border)
print(X_augmented.shape)

plot_data(X_augmented, y_augmented, n_rows=5)

X_diff = calculate_difference(X_train, X_augmented)
plot_data(X_diff, y_augmented, n_rows=2)

plot_as_one(X_diff, title="Difference Random Affine Transformations - Original", images_per_row=8)

#Control values to override the MC generated values for testing purposes:
def test_transform_boundary(transform='shears', transform_boundary=0.1, sample_size=40):
  
  #Set all affine transform to values which do not change original image
  scales = [1]*sample_size
  rotations = [0]*sample_size
  shears = [0]*sample_size
  shifts = [[0, 0]]*sample_size

  #Generate transform with boundary values
  if transform == 'shears':
    shears = [-transform_boundary, transform_boundary]*int(sample_size/2)
  elif transform == 'rotations':
    rotations = [-transform_boundary, transform_boundary]*int(sample_size/2)
  elif transform == 'scales':
    scales = [1-transform_boundary, 1+transform_boundary]*int(sample_size/2)
  else:
    shifts = [[-transform_boundary, transform_boundary], [-transform_boundary, transform_boundary]]*int(sample_size/2)

  return {'shears': shears, 'scales': scales,
          'rotations': rotations, 'shifts': shifts}

test_params = test_transform_boundary('shifts', -2)
for i, j in test_params.items():
  print(i, j)

X_augmented_test, y_augmented_test = apply_affine_transform(data=X_train, apply_to_all=True, **test_params)
plot_data(data=X_augmented_test, labels=y_augmented_test, n_rows=2, n_columns=8, col_map=True)

X_diff = calculate_difference(X_train, X_augmented_test)
plot_as_one(X_diff, title="Difference Affine Shift by [2, -2] - Original")

#Shift all images by 1 degree and plot their difference
test_params = test_transform_boundary('rotations', 1)

X_augmented_test, y_augmented_test = apply_affine_transform(data=X_train, apply_to_all=True, **test_params)
X_diff = calculate_difference(X_train, X_augmented_test)

plot_as_one(X_diff, title="Difference Affine Rotation by 1{} - Original".format(chr(176)))

#Shift all images by 5 degrees and plot their difference
test_params = test_transform_boundary('rotations', 5)

X_augmented_test, y_augmented_test = apply_affine_transform(data=X_train, apply_to_all=True, **test_params)
X_diff = calculate_difference(X_train, X_augmented_test)

plot_as_one(X_diff, title="Difference Affine Rotation by 5{} - Original".format(chr(176)))

from sklearn.linear_model import SGDClassifier
import time

sgd_clf0 = SGDClassifier(random_state=0)
t0 = time.time()
sgd_clf0.fit(X_train, y_train) 
t1 = time.time()
print("Score SGDClassifier on full dataset:", sgd_clf0.score(X_test, y_test))
print("Training time on full dataset:\t{:.3f}".format(t1-t0))

sgd_clf1 = SGDClassifier(random_state=0)
t0 = time.time()
sgd_clf1.fit(X_train[:denoised_border], y_train[:denoised_border])
t1 = time.time()
print("Score SGDClassifier on denoised dataset:", sgd_clf1.score(X_test, y_test))
print("Training time on denoised dataset:\t{:.3f}".format(t1-t0))

#Find best hyperparameters for the SGDClassifier on a :5k subset
from sklearn.model_selection import GridSearchCV

param_grid = {'penalty': ['l1', 'l2'],
              'alpha': [30, 100, 300, 1000, 3000]}

sgd_grid_search = GridSearchCV(SGDClassifier(random_state=0), param_grid)
t0 = time.time()
sgd_grid_search.fit(X_train[:denoised_border], y_train[:denoised_border])
t1 = time.time()
print("GridSearch time on denoised dataset:\t{:.3f}".format(t1-t0))

sgd_grid_search.best_estimator_

from sklearn.metrics import precision_score, recall_score

y_pred = sgd_clf0.predict(X_test)
pr = precision_score(y_test, y_pred, average='macro')
re = recall_score(y_test, y_pred, average='macro')

print("Precision score: \t", pr)
print("Recall score: \t", re)
print()

print("GridSearch best estimator's score: ", sgd_grid_search.best_estimator_.score(X_test, y_test))
y_pred = sgd_grid_search.best_estimator_.predict(X_test)
pr = precision_score(y_test, y_pred, average='macro')
re = recall_score(y_test, y_pred, average='macro')

print("Precision score: \t", pr)
print("Recall score: \t", re)
print()

#Create datasets with combined data
X_original_denoised = np.concatenate((X_train[:denoised_border], X_denoised))
y_original_denoised = np.concatenate((y_train[:denoised_border], y_denoised))
print("Orginal + Denoised shapes: \t", X_original_denoised.shape, y_original_denoised.shape)

X_original_augmented = np.concatenate((X_train[:denoised_border], X_augmented))
y_original_augmented = np.concatenate((y_train[:denoised_border], y_train[:denoised_border]))
print("Orginal + Affine shapes: \t", X_original_augmented.shape, y_original_augmented.shape)

X_original_denoised_augmented = np.concatenate((X_train[:denoised_border], X_denoised, X_augmented))
y_original_denoised_augmented = np.concatenate((y_train[:denoised_border], y_denoised, y_train[:denoised_border]))
print("Orginal + Denoised + Augmented shapes: \t", X_original_augmented.shape, y_original_augmented.shape)

data_list = [X_train[:denoised_border], X_denoised, X_augmented, X_original_denoised, X_original_augmented, X_original_denoised_augmented]
labels_list = [y_train[:denoised_border], y_denoised, y_train[:denoised_border], y_original_denoised, y_original_augmented, y_original_denoised_augmented]
dataset_names = ["Original MNIST", "Denoised", "Affine Augmented", "Original+Denoised", "Original+Augmented", "Original+Denoised+Augmented"]

#Fit and test a classifier on each dataset
for data, labels, name in zip(data_list, labels_list, dataset_names):
  print("Fitting ", name, "dataset.\nShape: \t{}".format(data.shape))
  sgd_clf = SGDClassifier(random_state=0)
  sgd_clf.fit(data, labels)
  y_pred = sgd_clf.predict(X_test)

  pr = precision_score(y_test, y_pred, average='macro')
  re = recall_score(y_test, y_pred, average='macro')

  print("Precision score: \t", pr)
  print("Recall score: \t", re)
  print()

def test_affine_transform(transform, values, clf=SGDClassifier(random_state=0),
                          transform_X_test=False, verbose=True):
  tested_values, precisions, recalls = [], [], []

  print(clf.__class__.__name__)

  for i in values:
    test_params2 = test_transform_boundary(transform, i, sample_size=denoised_border)
    X_augmented2, y_augmented2 = apply_affine_transform(data=X_train, **test_params2, sample_size=denoised_border)
    
    if transform_X_test:
      X_test2, _ = apply_affine_transform(data=X_test, labels=y_test, **test_params2, sample_size=denoised_border)
    else:
      X_test2, _ = X_test[:denoised_border], y_test[:denoised_border]
    y_test2 = y_test[:denoised_border]

    print("Fitting augmentation", transform, "by", i)
    t0 = time.time()
    clf.fit(X_augmented2, y_train[:denoised_border])
    t1 = time.time()
    y_pred2 = clf.predict(X_test2)

    t = t1 - t0
    pr = precision_score(y_test2, y_pred2, average='macro')
    re = recall_score(y_test2, y_pred2, average='macro')

    tested_values.append(i)
    precisions.append(pr)
    recalls.append(re)

    if verbose:
      print("Train data shape", X_augmented2.shape)
      print("Test data shape", X_test2.shape)
      print("Fitting time: \t", t)
      print("Precision score: \t", pr)
      print("Recall score: \t", re)
      print()

  return {"Tested transformation": transform, "Tested values": values,
          "Tested classifier": clf.__class__.__name__,
          "Resulting precisions": precisions, "Resulting recalls": recalls}

transforms_list = ["shears", "scales", "rotations", "shifts"]
values_list = [np.linspace(-0.5, 0.5, 11), np.linspace(-0.5, 0.5, 11), np.linspace(-5, 5, 11), np.linspace(-5, 5, 11)]

affine_tests = []
for transform, value in zip(transforms_list, values_list):
  affine_tests.append(test_affine_transform(transform, value))

def plot_test_results(affine_tests):
  fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 5))

  for i in range(4):
    axes[i].set_title(affine_tests[i]["Tested transformation"]+" with "+affine_tests[i]["Tested classifier"], fontsize=12)
    axes[i].plot(affine_tests[i]["Tested values"], affine_tests[i]["Resulting precisions"], label='precision')
    axes[i].plot(affine_tests[i]["Tested values"], affine_tests[i]["Resulting recalls"], label='recall')
    axes[i].set_ylim(0.0, 0.9)
    axes[i].legend()

plot_test_results(affine_tests)

affine_tests2 = []
for transform, value in zip(transforms_list, values_list):
  affine_tests2.append(test_affine_transform(transform, value, transform_X_test=True))

plot_test_results(affine_tests2)

#Evaluating multiple classifiers on original MNIST
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC, SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier

lgr_clf = LogisticRegression(random_state=0)
lin_svm_clf = LinearSVC(random_state=0)
svm_clf = SVC(kernel='rbf', gamma='scale', random_state=0)
mlp_clf = MLPClassifier(random_state=0)
forest_clf = RandomForestClassifier(n_estimators=100, random_state=0)
extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=0)
gboost_clf = GradientBoostingClassifier(random_state=0)
knn_clf = KNeighborsClassifier(n_neighbors=50)

classifiers = [lgr_clf, lin_svm_clf, svm_clf, mlp_clf, forest_clf, extra_trees_clf, gboost_clf, knn_clf]

print("Train data shape", X_train.shape)
print("Test data shape", X_test.shape)

clf_names, clf_times, clf_precisions, clf_recalls = [], [], [], []

for clf in classifiers:
  name = clf.__class__.__name__
  clf_names.append(name)
  print("Fitting", name)

  t0 = time.time()
  clf.fit(X_train, y_train)
  t1 = time.time()
  t = t1 - t0
  clf_times.append(t)
  print("Fitting time: {:.3f}".format(t))

  y_pred = clf.predict(X_test)

  pr = precision_score(y_test, y_pred, average='macro')
  clf_precisions.append(pr)
  print("Precision score: \t", pr)

  re = recall_score(y_test, y_pred, average='macro')
  clf_recalls.append(re)
  print("Recall score: \t", re)
  print()

plt.figure(figsize=(20, 5))

y_pos = np.arange(len(clf_names))
names_short = ['LR', 'LinSVC', 'SVC', 'MLP', 'RF', 'ExtraT', 'GBoost', 'KNN']

plt.subplot(131)
plt.bar(y_pos, clf_times, align='center', alpha=0.5)
plt.xticks(y_pos, names_short)
plt.ylabel("seconds")
plt.title("Fitting times")

plt.subplot(132)
plt.bar(y_pos, clf_precisions, align='center', alpha=0.5)
plt.plot([-0.5, 7.5], [0.95, 0.95], 'r--')
plt.xticks(y_pos, names_short)
plt.title("Precisions with 95% benchmark")

plt.subplot(133)
plt.bar(y_pos, clf_recalls, align='center', alpha=0.5)
plt.plot([-0.5, 7.5], [0.95, 0.95], 'r--')
plt.xticks(y_pos, names_short)
plt.title("Recalls with 95% benchmark")

classifiers_short = [lgr_clf, lin_svm_clf, mlp_clf, forest_clf]

for clf in classifiers_short:
  affine_tests3 = []
  for transform, value in zip(transforms_list, values_list):
    affine_tests3.append(test_affine_transform(transform, value, clf))

  plot_test_results(affine_tests3)

#Create voting classifier fro the quickest classifiers with pr, re>95%
from sklearn.ensemble import VotingClassifier

named_estimators = [
    ("forest_clf", RandomForestClassifier(n_estimators=100, random_state=0)),
    ("extra_clf", ExtraTreesClassifier(n_estimators=100, random_state=0)),
    ("mlp_clf", MLPClassifier(random_state=0)),
]

voting_clf = VotingClassifier(named_estimators)
print("Fitting", voting_clf.__class__.__name__,)

t0 = time.time()
voting_clf.fit(X_train, y_train)
t1 = time.time()
print("Fitting time: {:.3f}".format(t1-t0))

y_pred = voting_clf.predict(X_test)
pr = precision_score(y_test, y_pred, average='macro')
re = recall_score(y_test, y_pred, average='macro')

print("Precision score: \t", pr)
print("Recall score: \t", re)
print()

#Better than all of MLP, RandomForest and ExtraTrees individually
#time = 193.35, precision=0.9738, recall=0.9738

#Create voting classifier adding SVC (best, but slow)
named_estimators2 = [
    ("forest_clf", RandomForestClassifier(n_estimators=100, random_state=0)),
    ("extra_clf", ExtraTreesClassifier(n_estimators=100, random_state=0)),
    ("mlp_clf", MLPClassifier(random_state=0)),
    ("svc_clf", SVC(kernel='rbf', gamma='scale', random_state=0)),]

voting_clf2 = VotingClassifier(named_estimators2)
print("Fitting", voting_clf2.__class__.__name__,)

t0 = time.time()
voting_clf2.fit(X_train, y_train)
t1 = time.time()
print("Fitting time: {:.3f}".format(t1-t0))

y_pred = voting_clf2.predict(X_test)
pr = precision_score(y_test, y_pred, average='macro')
re = recall_score(y_test, y_pred, average='macro')

print("Precision score: \t", pr)
print("Recall score: \t", re)
print()

#Worse than kernel SVC only
#time = 657.28, precision=0.9773, recall=0.9772

#Create a voting classifier from previous voting classifier and SVC
named_estimators3 = [
    ("voting_clf", VotingClassifier(named_estimators)),
    ("svc_clf", SVC(kernel='rbf', gamma='scale', random_state=0)),]

voting_clf3 = VotingClassifier(named_estimators3)
print("Fitting", voting_clf3.__class__.__name__,)

t0 = time.time()
voting_clf3.fit(X_train, y_train)
t1 = time.time()
print("Fitting time: {:.3f}".format(t1-t0))

y_pred = voting_clf3.predict(X_test)
pr = precision_score(y_test, y_pred, average='macro')
re = recall_score(y_test, y_pred, average='macro')

print("Precision score: \t", pr)
print("Recall score: \t", re)
print()

#Worse the previous voting classifier, better than first
#time=658.37, precision=0.9764, recall=0.9761